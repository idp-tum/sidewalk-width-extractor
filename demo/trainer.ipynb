{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sidewalk_widths_extractor import Trainer, seed_all\n",
    "from sidewalk_widths_extractor.dataset import SateliteDataset\n",
    "from sidewalk_widths_extractor.modules.test import TestModule\n",
    "from sidewalk_widths_extractor.utilities import get_device\n",
    "\n",
    "seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"logs//demo\"\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VAL_BATCH_SIZE = 4\n",
    "NUM_WORKERS = 2\n",
    "PERSISTENT_WORKERS = True\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "device = get_device()\n",
    "opt_params = {\"lr\": 2e-4, \"weight_decay\": 1e-4}\n",
    "module = TestModule(opt_params, device)\n",
    "\n",
    "dataset = SateliteDataset(\"data/images/\", \"data/masks/\")\n",
    "train_size = int(SPLIT_RATIO * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=PERSISTENT_WORKERS,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=PERSISTENT_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1] Training: 100% 1/1 [00:16<00:00, 16.01s/it]\n",
      "[1] Validating: 100% 1/1 [00:05<00:00,  5.80s/it]\n",
      "[2] Training: 100% 1/1 [00:00<00:00,  1.08it/s]\n",
      "[2] Validating: 100% 1/1 [00:00<00:00,  2.42it/s]\n",
      "[3] Training: 100% 1/1 [00:00<00:00,  1.10it/s]\n",
      "[3] Validating: 100% 1/1 [00:00<00:00,  2.29it/s]\n",
      "[4] Training: 100% 1/1 [00:00<00:00,  1.31it/s]\n",
      "[4] Validating: 100% 1/1 [00:00<00:00,  3.05it/s]\n",
      "[5] Training: 100% 1/1 [00:00<00:00,  1.55it/s]\n",
      "[5] Validating: 100% 1/1 [00:00<00:00,  2.33it/s]\n",
      "[6] Training: 100% 1/1 [00:00<00:00,  1.48it/s]\n",
      "[6] Validating: 100% 1/1 [00:00<00:00,  1.75it/s]\n",
      "[7] Training: 100% 1/1 [00:00<00:00,  1.33it/s]\n",
      "[7] Validating: 100% 1/1 [00:00<00:00,  2.00it/s]\n",
      "[8] Training: 100% 1/1 [00:00<00:00,  1.09it/s]\n",
      "[8] Validating: 100% 1/1 [00:00<00:00,  1.92it/s]\n",
      "[9] Training: 100% 1/1 [00:00<00:00,  1.54it/s]\n",
      "[9] Validating: 100% 1/1 [00:00<00:00,  1.81it/s]\n",
      "[10] Training: 100% 1/1 [00:00<00:00,  1.26it/s]\n",
      "[10] Validating: 100% 1/1 [00:00<00:00,  1.66it/s]\n",
      "[11] Training: 100% 1/1 [00:00<00:00,  1.41it/s]\n",
      "[11] Validating: 100% 1/1 [00:00<00:00,  2.53it/s]\n",
      "[12] Training: 100% 1/1 [00:00<00:00,  1.25it/s]\n",
      "[12] Validating: 100% 1/1 [00:00<00:00,  2.51it/s]\n",
      "[13] Training: 100% 1/1 [00:00<00:00,  1.27it/s]\n",
      "[13] Validating: 100% 1/1 [00:00<00:00,  2.14it/s]\n",
      "[14] Training: 100% 1/1 [00:01<00:00,  1.11s/it]\n",
      "[14] Validating: 100% 1/1 [00:00<00:00,  1.93it/s]\n",
      "[15] Training: 100% 1/1 [00:00<00:00,  1.05it/s]\n",
      "[15] Validating: 100% 1/1 [00:00<00:00,  2.07it/s]\n",
      "[16] Training: 100% 1/1 [00:01<00:00,  1.04s/it]\n",
      "[16] Validating: 100% 1/1 [00:00<00:00,  1.53it/s]\n",
      "[17] Training: 100% 1/1 [00:00<00:00,  1.02it/s]\n",
      "[17] Validating: 100% 1/1 [00:00<00:00,  1.94it/s]\n",
      "[18] Training: 100% 1/1 [00:01<00:00,  1.05s/it]\n",
      "[18] Validating: 100% 1/1 [00:00<00:00,  2.10it/s]\n",
      "[19] Training: 100% 1/1 [00:00<00:00,  1.37it/s]\n",
      "[19] Validating: 100% 1/1 [00:00<00:00,  2.30it/s]\n",
      "[20] Training: 100% 1/1 [00:01<00:00,  1.17s/it]\n",
      "[20] Validating: 100% 1/1 [00:00<00:00,  2.43it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(override_log_dir=LOG_DIR)\n",
    "trainer.fit(\n",
    "    module=module,\n",
    "    dataloader=train_dataloader,\n",
    "    validate_dataloader=val_dataloader,\n",
    "    max_epochs=20,\n",
    "    save_every_n_epoch=5,\n",
    "    save_settings=True,\n",
    "    save_scalars=True,\n",
    "    save_figures=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21] Training: 100% 1/1 [00:00<00:00,  1.00it/s]\n",
      "[21] Validating: 100% 1/1 [00:00<00:00,  2.00it/s]\n",
      "[22] Training: 100% 1/1 [00:00<00:00,  1.36it/s]\n",
      "[22] Validating: 100% 1/1 [00:00<00:00,  2.16it/s]\n",
      "[23] Training: 100% 1/1 [00:00<00:00,  1.16it/s]\n",
      "[23] Validating: 100% 1/1 [00:00<00:00,  2.46it/s]\n",
      "[24] Training: 100% 1/1 [00:00<00:00,  1.60it/s]\n",
      "[24] Validating: 100% 1/1 [00:00<00:00,  2.26it/s]\n",
      "[25] Training: 100% 1/1 [00:00<00:00,  1.08it/s]\n",
      "[25] Validating: 100% 1/1 [00:00<00:00,  2.38it/s]\n"
     ]
    }
   ],
   "source": [
    "path = {\n",
    "    \"network\": os.path.join(LOG_DIR, \"checkpoints//20//network.pth.tar\"),\n",
    "    \"optimizer\": os.path.join(LOG_DIR, \"checkpoints//20//optimizer.pth.tar\"),\n",
    "}\n",
    "\n",
    "trainer = Trainer(override_log_dir=LOG_DIR)\n",
    "trainer.fit(\n",
    "    module,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    max_epochs=5,\n",
    "    checkpoint_path=path,\n",
    "    save_scalars=True,\n",
    "    save_figures=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100% 1/1 [00:00<00:00, 21.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [tensor(0.6908, device='cuda:0')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if there is no prior training happened\n",
    "# trainer = Trainer() \n",
    "\n",
    "path = {\n",
    "    \"network\": os.path.join(LOG_DIR, \"checkpoints//25//network.pth.tar\"),\n",
    "    \"optimizer\": os.path.join(LOG_DIR, \"checkpoints//25//optimizer.pth.tar\"),\n",
    "}\n",
    "\n",
    "results = trainer.validate(\n",
    "    val_dataloader,\n",
    "    module,\n",
    "    checkpoint_path=path,\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100% 1/1 [00:00<00:00, 20.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [tensor(0.6908, device='cuda:0')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if there is no prior training happened\n",
    "# trainer = Trainer() \n",
    "\n",
    "path = {\n",
    "    \"network\": os.path.join(LOG_DIR, \"checkpoints//25//network.pth.tar\"),\n",
    "    \"optimizer\": os.path.join(LOG_DIR, \"checkpoints//25//optimizer.pth.tar\"),\n",
    "}\n",
    "\n",
    "results = trainer.test(\n",
    "    val_dataloader,\n",
    "    module,\n",
    "    checkpoint_path=path,\n",
    ")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a6a66adaf5fb32f581cec0ac67396c0be9b6783f51fd9dfef301bb5ed0758428"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
