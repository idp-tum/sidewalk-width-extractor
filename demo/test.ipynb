{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from functools import reduce\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "from sidewalk_widths_extractor import Trainer, seed_all\n",
    "from sidewalk_widths_extractor.dataset import SatelliteDataset\n",
    "from sidewalk_widths_extractor.modules.seg import SegModule\n",
    "from sidewalk_widths_extractor.utilities import get_device\n",
    "from sidewalk_widths_extractor.utilities.io import mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER_PATH = \"data//images\"\n",
    "MASK_FOLDER_PATH = \"data//masks\"\n",
    "TARGET_LOG_FOLDER = \"logs//test\"\n",
    "\n",
    "EXTRACT_PREDICTIONS = True\n",
    "\n",
    "batch_size = 8\n",
    "shuffle = False\n",
    "drop_last_batch = True\n",
    "pin_memory = True\n",
    "num_workers = 0\n",
    "persistent_workers = False\n",
    "\n",
    "device = get_device()\n",
    "print(\"using\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_models = [\n",
    "    {\n",
    "        \"model\": \"logs//hpt//18-09-2022 11-49-51 hpt 0//checkpoints//best_network.pth.tar\",\n",
    "        \"settings\": \"logs//hpt//18-09-2022 11-49-51 hpt 0//settings.json\",\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"logs//hpt//18-09-2022 11-51-13 hpt 1//checkpoints//best_network.pth.tar\",\n",
    "        \"settings\": \"logs//hpt//18-09-2022 11-51-13 hpt 1//settings.json\",\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"logs//hpt//18-09-2022 11-52-26 hpt 2//checkpoints//best_network.pth.tar\",\n",
    "        \"settings\": \"logs//hpt//18-09-2022 11-52-26 hpt 2//settings.json\",\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"logs//hpt//18-09-2022 11-53-42 hpt 3//checkpoints//best_network.pth.tar\",\n",
    "        \"settings\": \"logs//hpt//18-09-2022 11-53-42 hpt 3//settings.json\",\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"logs//hpt//18-09-2022 11-55-04 hpt 4//checkpoints//best_network.pth.tar\",\n",
    "        \"settings\": \"logs//hpt//18-09-2022 11-55-04 hpt 4//settings.json\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, m in enumerate(target_models):\n",
    "    model_path = m[\"model\"]\n",
    "    settings_path = m[\"settings\"]\n",
    "\n",
    "    assert os.path.exists(model_path)\n",
    "    assert os.path.exists(settings_path)\n",
    "\n",
    "    settings = None\n",
    "    with open(settings_path) as file:\n",
    "        settings = json.load(file)\n",
    "\n",
    "    assert isinstance(settings[\"module\"][\"network\"], dict)\n",
    "    assert isinstance(settings[\"module\"][\"optimizer\"], dict)\n",
    "    assert isinstance(settings[\"module\"][\"criterion\"], dict)\n",
    "\n",
    "    run_id = settings[\"run\"][\"run_id\"]\n",
    "    no_train_samples = settings[\"run\"][\"no_train_samples\"]\n",
    "    no_parameters = settings[\"module\"][\"network\"][\"no_parameters\"]\n",
    "\n",
    "    seed_all()\n",
    "\n",
    "    dataset = SatelliteDataset(IMAGE_FOLDER_PATH, MASK_FOLDER_PATH)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last_batch,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=persistent_workers,\n",
    "    )\n",
    "    module = SegModule(\n",
    "        settings[\"module\"][\"network\"][\"id\"],\n",
    "        settings[\"module\"][\"network\"][\"params\"],\n",
    "        settings[\"module\"][\"optimizer\"][\"id\"],\n",
    "        settings[\"module\"][\"optimizer\"][\"params\"],\n",
    "        settings[\"module\"][\"criterion\"][\"id\"],\n",
    "        settings[\"module\"][\"criterion\"][\"params\"],\n",
    "        device=device,\n",
    "        save_network_checkpoint=False,\n",
    "        save_optimizer_checkpoint=False,\n",
    "    )\n",
    "\n",
    "    module.load({\"network\": model_path})\n",
    "    epoch = module.curr_epoch_idx\n",
    "\n",
    "    trainer = Trainer(TARGET_LOG_FOLDER, progress_bar=False, transfer_results_to_cpu=True)\n",
    "\n",
    "    result = trainer.test(dataloader, module)\n",
    "\n",
    "    tp = sum(result[\"tp\"])\n",
    "    fp = sum(result[\"fp\"])\n",
    "    fn = sum(result[\"fn\"])\n",
    "    tn = sum(result[\"tn\"])\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "    if not tp + fp == 0:\n",
    "        precision = tp / (tp + fp)\n",
    "    else:\n",
    "        precision = torch.tensor(0.0)\n",
    "\n",
    "    if not tp + fn == 0:\n",
    "        recall = tp / (tp + fn)\n",
    "    else:\n",
    "        recall = torch.tensor(0.0)\n",
    "\n",
    "    f1 = (\n",
    "        2 * (precision * recall) / (precision + recall)\n",
    "        if precision + recall != 0.0\n",
    "        else torch.tensor(0.0)\n",
    "    )\n",
    "\n",
    "    iou = tp / (tp + fn + fp)\n",
    "\n",
    "    dice = 2 * tp / (2 * tp + fn + fp)\n",
    "\n",
    "    result = {\n",
    "        \"id\": [run_id],\n",
    "        \"epoch\": [epoch],\n",
    "        \"no_train_samples\": [no_train_samples],\n",
    "        \"no_parameters\": [no_parameters],\n",
    "        \"tp\": [tp.item()],\n",
    "        \"fp\": [fp.item()],\n",
    "        \"fn\": [fn.item()],\n",
    "        \"tn\": [tn.item()],\n",
    "        \"accuracy\": [accuracy.item()],\n",
    "        \"precision\": [precision.item()],\n",
    "        \"recall\": [recall.item()],\n",
    "        \"f1\": [f1.item()],\n",
    "        \"iou\": [iou.item()],\n",
    "        \"dice\": [dice.item()],\n",
    "    }\n",
    "\n",
    "    run_target_path = os.path.join(TARGET_LOG_FOLDER, run_id)\n",
    "    mkdir(run_target_path)\n",
    "    with open(os.path.join(run_target_path, \"settings.json\"), \"w\") as file:\n",
    "        json.dump(settings, file, indent=2, sort_keys=False)\n",
    "\n",
    "    results.append(pd.DataFrame(result, index=[i]))\n",
    "\n",
    "    if EXTRACT_PREDICTIONS:\n",
    "        target_pred_folder = os.path.join(run_target_path, \"predictions\")\n",
    "        mkdir(target_pred_folder)\n",
    "\n",
    "        image_paths = [x for x in sorted(glob(os.path.join(IMAGE_FOLDER_PATH, \"*\")))]\n",
    "        mask_paths = [x for x in sorted(glob(os.path.join(MASK_FOLDER_PATH, \"*\")))]\n",
    "\n",
    "        tensor_transform = T.ToTensor()\n",
    "        image_transform = T.ToPILImage()\n",
    "\n",
    "        for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "            target_img_path = os.path.join(\n",
    "                target_pred_folder, os.path.splitext(os.path.basename(img_path))[0]\n",
    "            )\n",
    "\n",
    "            mkdir(target_img_path)\n",
    "\n",
    "            full_image = cv2.imread(img_path)\n",
    "            full_image = cv2.cvtColor(full_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            full_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            # full_mask = np.where(full_mask == 255, 1, 0)\n",
    "\n",
    "            h, w, _ = full_image.shape\n",
    "\n",
    "            if h == 512 and w == 512:\n",
    "                images = [\n",
    "                    full_image[: h // 2, : w // 2],\n",
    "                    full_image[: h // 2, w // 2 :],\n",
    "                    full_image[h // 2 :, : w // 2],\n",
    "                    full_image[h // 2 :, w // 2 :],\n",
    "                ]\n",
    "                masks = [\n",
    "                    full_mask[: h // 2, : w // 2],\n",
    "                    full_mask[: h // 2, w // 2 :],\n",
    "                    full_mask[h // 2 :, : w // 2],\n",
    "                    full_mask[h // 2 :, w // 2 :],\n",
    "                ]\n",
    "            else:\n",
    "                images = [full_image]\n",
    "                masks = [full_mask]\n",
    "\n",
    "            for i, (x, y) in enumerate(zip(images, masks)):\n",
    "                image = tensor_transform(x)\n",
    "                truth = torch.from_numpy(y)\n",
    "                pred = module.infer(image).detach().cpu()\n",
    "\n",
    "                image = (image * 255).type(torch.uint8)\n",
    "                pred = pred.type(torch.bool)\n",
    "                # truth = truth.type(torch.bool)\n",
    "\n",
    "                seg = draw_segmentation_masks(image, pred, alpha=0.3, colors=\"blue\")\n",
    "\n",
    "                image_transform(seg).save(os.path.join(target_img_path, f\"seg_{i}.png\"))\n",
    "                image_transform((pred * 255).type(torch.uint8)).save(\n",
    "                    os.path.join(target_img_path, f\"pred_mask{i}.png\")\n",
    "                )\n",
    "                image_transform(truth).save(os.path.join(target_img_path, f\"truth_mask_{i}.png\"))\n",
    "\n",
    "out = pd.concat(results)\n",
    "out.to_csv(os.path.join(TARGET_LOG_FOLDER, \"results.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('idp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6a66adaf5fb32f581cec0ac67396c0be9b6783f51fd9dfef301bb5ed0758428"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
